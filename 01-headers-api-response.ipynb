{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Prerequisites\n",
    "\n",
    "Ensure that your Azure Services are properly set up, your Conda environment is created, and your environment variables are configured as per the instructions in the [SETTINGS.md](SETTINGS.md) file.\n",
    "\n",
    "## ðŸ“‹ Table of Contents\n",
    "\n",
    "This notebook assists in testing and retrieving the headers of Azure OpenAI, covering the following sections:\n",
    "\n",
    "1. [**Setting Up Azure OpenAI Client**](#setting-up-azure-openai-client): Outlines the process of initializing the Azure OpenAI client.\n",
    "\n",
    "2. [**Calling Azure OpenAI API**](#calling-azure-openai-api): Discusses how to make API calls to Azure OpenAI.\n",
    "\n",
    "3. [**Extracting Headers and Payload Metadata**](#extracting-headers-and-payload-metadata): Explores how to extract headers and payload metadata from the API response.\n",
    "\n",
    "4. [**Analyzing Rate Limit Info**](#analyzing-rate-limit-info): Details the steps to analyze the rate limit information from the API response.\n",
    "\n",
    "For additional information, refer to the following resources:\n",
    "- [AOAI API Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\gbbai-azure-aoai-faq\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = (\n",
    "    r\"C:\\Users\\pablosal\\Desktop\\gbbai-azure-aoai-faq\"  # change your directory here\n",
    ")\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from dotenv import load_dotenv\n",
    "from src.aoai.azure_openai import AzureOpenAIManager\n",
    "from typing import Dict, Optional\n",
    "from requests import Response\n",
    "import requests\n",
    "\n",
    "from utils.ml_logging import get_logger\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Set up logger\n",
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Azure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the client. You can find it in src/aoai/azure_openai.py.\n",
    "# It is essentially a wrapper using dependency injection to automate the initialization\n",
    "# and most used API calls.\n",
    "azure_openai_client = AzureOpenAIManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling Azure OpenAI API\n",
    "\n",
    "### Creating Helper Function\n",
    "This function is designed to extract and build the metadata in JSON format. It's particularly useful for extracting rate limit and usage information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rate_limit_and_usage_info(response: Response) -> Dict[str, Optional[int]]:\n",
    "    \"\"\"\n",
    "    Extracts rate limiting information from the Azure Open API response headers and usage information from the payload.\n",
    "\n",
    "    :param response: The response object returned by a requests call.\n",
    "    :return: A dictionary containing the remaining requests, remaining tokens, and usage information\n",
    "             including prompt tokens, completion tokens, and total tokens.\n",
    "    \"\"\"\n",
    "    headers = response.headers\n",
    "    usage = response.json().get(\"usage\", {})\n",
    "    return {\n",
    "        \"remaining-requests\": headers.get(\"x-ratelimit-remaining-requests\"),\n",
    "        \"remaining-tokens\": headers.get(\"x-ratelimit-remaining-tokens\"),\n",
    "        \"prompt-tokens\": usage.get(\"prompt_tokens\"),\n",
    "        \"completion_tokens\": usage.get(\"completion_tokens\"),\n",
    "        \"total_tokens\": usage.get(\"total_tokens\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encapsulating Completions API Calls into a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fa4384cb235b4b4781ee2af4c997f37b'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_openai_client.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_azure_openai_chat_completions_api(\n",
    "    deployment_id: str, method: str, body: dict = None, api_version: str = \"2023-11-01\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the Azure OpenAI API with the given parameters.\n",
    "\n",
    "    :param deployment_id: The ID of the deployment to access.\n",
    "    :param method: The HTTP method to use (\"get\" or \"post\").\n",
    "    :param body: The body of the request for \"post\" method. Defaults to None.\n",
    "    :param api_version: The API version to use. Defaults to \"2023-11-01\".\n",
    "\n",
    "    :return: The status code and response from the API call, along with rate limit headers.\n",
    "    \"\"\"\n",
    "    if method.lower() not in [\"get\", \"post\"]:\n",
    "        logger.error(\"Invalid HTTP method. Expected 'get' or 'post'.\")\n",
    "        return None, None, {}\n",
    "\n",
    "    url = f\"{azure_openai_client.azure_endpoint}/openai/deployments/{deployment_id}/chat/completions?api-version={api_version}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": azure_openai_client.api_key,\n",
    "    }\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update(headers)\n",
    "\n",
    "        try:\n",
    "            if method.lower() == \"get\":\n",
    "                response = session.get(url)\n",
    "            else:  # method.lower() == \"post\"\n",
    "                response = session.post(url, json=body)\n",
    "            response.raise_for_status()  # Raises HTTPError for bad responses\n",
    "        except requests.HTTPError as http_err:\n",
    "            logger.error(f\"HTTP error occurred: {http_err}\")\n",
    "            return response.status_code, http_err.response.json(), {}\n",
    "        except Exception as err:\n",
    "            logger.error(f\"An error occurred: {err}\")\n",
    "            return None, None, {}\n",
    "\n",
    "    # Extract rate limit headers and usage details\n",
    "    rate_limit_headers = extract_rate_limit_and_usage_info(response)\n",
    "    return response.status_code, response.json(), rate_limit_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Headers and Payload Metadata\n",
    "\n",
    "#### Constructing the Request\n",
    "\n",
    "- **max_tokens**: Optional. Integer specifying the maximum number of tokens to generate. Default is 24.\n",
    "- **temperature**: Optional. Number between 0 and 2 indicating the sampling temperature. Default is 1.\n",
    "- **top_p**: Optional. Nucleus sampling parameter as a number between 0 and 1. Default is 1.\n",
    "- **user**: Optional. A unique identifier for the end-user to help monitor and detect abuse.\n",
    "- **n**: Optional. Integer for the number of completions to generate for each prompt. Default is 1.\n",
    "- **presence_penalty**: Optional. Number between -2.0 and 2.0 to penalize new tokens based on presence in the text so far. Default is 0.\n",
    "- **frequency_penalty**: Optional. Number between -2.0 and 2.0 to penalize new tokens based on frequency in the text so far. Default is 0.\n",
    "- **messages**: Optional. An array of message objects.\n",
    "\n",
    "You can learn more about the aoai API  [official documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "    \"max_tokens\": 24,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 1,\n",
    "    \"user\": \"\",\n",
    "    \"n\": 1,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Does Azure OpenAI support customer managed keys?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Yes, customer managed keys are supported by Azure OpenAI.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Do other Azure AI services support this too?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Yes, other Azure AI services also support customer managed keys.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Can you tell me more about these services?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Sure, Azure AI services include Azure Cognitive Services, Azure Machine Learning, and more.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is Azure Cognitive Services?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Azure Cognitive Services is a collection of APIs and services for building intelligent applications.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is Azure Machine Learning?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Azure Machine Learning is a cloud-based service for building, training, and deploying machine learning models.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Thank you for the information.\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"You're welcome! If you have any other questions, feel free to ask.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What other services does Azure offer?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Azure offers a wide range of services including computing, analytics, storage, and networking.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you tell me more about Azure's computing services?\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Azure's computing services include virtual machines, container services, and serverless computing.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is serverless computing?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Serverless computing is a cloud computing model where the cloud provider automatically manages the provisioning and scaling of servers.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"That's interesting. Thank you for the information.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"You're welcome! If you have any other questions, feel free to ask.\",\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: {'id': 'chatcmpl-8m1ZcQYr6DQai7vYeqaG6vNFJurmm', 'object': 'chat.completion', 'created': 1706456484, 'model': 'gpt-4', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': \"If you have any more questions or need further assistance, just let me know! I'm here to help.\"}}], 'usage': {'prompt_tokens': 333, 'completion_tokens': 22, 'total_tokens': 355}, 'system_fingerprint': 'fp_6d044fb900'}\n",
      "Rate Limit Info: {'remaining-requests': '9', 'remaining-tokens': '9629', 'prompt-tokens': 333, 'completion_tokens': 22, 'total_tokens': 355}\n"
     ]
    }
   ],
   "source": [
    "status_code, response, rate_limit_info = call_azure_openai_chat_completions_api(\n",
    "    deployment_id=azure_openai_client.chat_model_name,\n",
    "    method=\"post\",\n",
    "    body=body,\n",
    "    api_version=\"2023-05-15\",\n",
    ")\n",
    "\n",
    "# Print the status code, response, and rate limit info\n",
    "print(\"Status Code:\", status_code)\n",
    "print(\"Response:\", response)\n",
    "print(\"Rate Limit Info:\", rate_limit_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Rate Limit Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rate limit information provides details about the usage of the API:\n",
    "\n",
    "- **Remaining Requests**: The number of API calls that can still be made. In this case, there are 9 requests left.\n",
    "- **Remaining Tokens**: The number of tokens that can still be generated. In this case, there are 9258 tokens left.\n",
    "- **Prompt Tokens**: The number of tokens used in the prompt for this API call. In this case, 333 tokens were used.\n",
    "- **Completion Tokens**: The number of tokens generated in the completion for this API call. In this case, 24 tokens were generated.\n",
    "- **Total Tokens**: The total number of tokens used in this API call. This is the sum of the prompt tokens and the completion tokens. In this case, 357 tokens were used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Programmatically Stop Call if Input Tokens Exceed Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, List, Dict\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "def num_tokens_from_input(\n",
    "    input: Union[str, List[Dict[str, str]]],\n",
    "    encoding_name: Optional[str] = \"cl100k_base\",\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Returns the total number of tokens in the input using the specified encoding.\n",
    "\n",
    "    This function uses the Tiktoken library to count the number of tokens in the input.\n",
    "    The input can be either a string or a list of dictionaries representing a conversation.\n",
    "    Each message in the conversation should be a dictionary with a \"content\" key containing the message text.\n",
    "    The encoding used for tokenization can be specified. If no encoding is specified,\n",
    "    \"cl100k_base\" is used by default.\n",
    "\n",
    "    Parameters:\n",
    "    input (Union[str, List[Dict[str, str]]]): The input to count tokens in.\n",
    "    encoding_name (Optional[str]): The name of the encoding to use for tokenization.\n",
    "                                   Defaults to \"cl100k_base\".\n",
    "\n",
    "    Returns:\n",
    "    int: The total number of tokens in the input.\n",
    "    \"\"\"\n",
    "\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "\n",
    "    if isinstance(input, str):\n",
    "        total_tokens = len(encoding.encode(input))\n",
    "    elif isinstance(input, list):\n",
    "        total_tokens = sum(\n",
    "            len(encoding.encode(message[\"content\"])) for message in input\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Input must be either a string or a list of dictionaries representing a conversation.\"\n",
    "        )\n",
    "\n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, Any, Optional, Union\n",
    "\n",
    "\n",
    "def call_azure_openai_chat_completions_api_with_pre_check(\n",
    "    deployment_id: str,\n",
    "    method: str,\n",
    "    body: Optional[Dict[str, Any]] = None,\n",
    "    api_version: str = \"2023-11-01\",\n",
    ") -> Tuple[Optional[int], Optional[Union[Dict[str, Any], str]], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Calls the Azure OpenAI API with the given parameters.\n",
    "\n",
    "    This function performs a pre-insertion check to ensure that there are enough tokens remaining for the request.\n",
    "    It uses the Tiktoken library to count the number of tokens in the request body and compares this with the remaining tokens from the rate limit info.\n",
    "    If there are not enough tokens remaining, the function aborts the request and returns an error message.\n",
    "\n",
    "    :param deployment_id: The ID of the deployment to access.\n",
    "    :param method: The HTTP method to use (\"get\" or \"post\").\n",
    "    :param body: The body of the request for \"post\" method. Defaults to None.\n",
    "    :param api_version: The API version to use. Defaults to \"2023-11-01\".\n",
    "\n",
    "    :return: The status code and response from the API call, along with rate limit headers.\n",
    "    \"\"\"\n",
    "    if method.lower() not in [\"get\", \"post\"]:\n",
    "        logger.error(\"Invalid HTTP method. Expected 'get' or 'post'.\")\n",
    "        return None, \"Invalid HTTP method. Expected 'get' or 'post'.\", {}\n",
    "    # Pre-Check\n",
    "    body_pre = {\n",
    "        \"max_tokens\": 1,\n",
    "        \"messages\": [{\"role\": \"assistant\", \"content\": \"\"}],\n",
    "    }\n",
    "    _, _, rate_limit_info = call_azure_openai_chat_completions_api(\n",
    "        deployment_id, method=method, body=body_pre, api_version=api_version\n",
    "    )\n",
    "    logger.info(f\"Rate Limit Info: {rate_limit_info}\")\n",
    "\n",
    "    tokens_needed_for_request = (\n",
    "        num_tokens_from_input(body[\"messages\"], \"cl100k_base\") if body else 0\n",
    "    )\n",
    "    logger.info(f\"Tokens Needed for Request: {tokens_needed_for_request}\")\n",
    "\n",
    "    if int(rate_limit_info[\"remaining-tokens\"]) >= tokens_needed_for_request:\n",
    "        logger.info(\n",
    "            f\"Enough tokens remaining, proceed with the request. Remaining tokens: {rate_limit_info['remaining-tokens']}, Tokens needed for request: {tokens_needed_for_request}\"\n",
    "        )\n",
    "        (\n",
    "            status_code,\n",
    "            response,\n",
    "            rate_limit_headers,\n",
    "        ) = call_azure_openai_chat_completions_api(\n",
    "            deployment_id, method=method, body=body, api_version=api_version\n",
    "        )\n",
    "        return status_code, response, rate_limit_headers\n",
    "    else:\n",
    "        logger.error(\n",
    "            f\"Not enough tokens remaining for the request. Remaining tokens: {rate_limit_info['remaining-tokens']}, Tokens needed for request: {tokens_needed_for_request}\"\n",
    "        )\n",
    "        return (\n",
    "            None,\n",
    "            \"Not enough tokens remaining, aborting or delaying the request.\",\n",
    "            {},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-28 09:41:27,433 - micro - MainProcess - INFO     Rate Limit Info: {'remaining-requests': '119', 'remaining-tokens': '119999', 'prompt-tokens': 7, 'completion_tokens': 1, 'total_tokens': 8} (1966967700.py:call_azure_openai_chat_completions_api_with_pre_check:35)\n",
      "2024-01-28 09:41:27,690 - micro - MainProcess - INFO     Tokens Needed for Request: 246 (1966967700.py:call_azure_openai_chat_completions_api_with_pre_check:40)\n",
      "2024-01-28 09:41:27,690 - micro - MainProcess - INFO     Enough tokens remaining, proceed with the request. Remaining tokens: 119999, Tokens needed for request: 246 (1966967700.py:call_azure_openai_chat_completions_api_with_pre_check:43)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: {'id': 'chatcmpl-8m1ZgxWEW7Wp1FPUJqSsktID80hfc', 'object': 'chat.completion', 'created': 1706456488, 'model': 'gpt-35-turbo-16k', 'choices': [{'finish_reason': 'length', 'index': 0, 'message': {'role': 'assistant', 'content': 'Sure! Azure OpenAI supports customer managed keys. With customer managed keys, you have control over the encryption keys used to'}}], 'usage': {'prompt_tokens': 333, 'completion_tokens': 24, 'total_tokens': 357}}\n",
      "Rate Limit Info: {'remaining-requests': '118', 'remaining-tokens': '119975', 'prompt-tokens': 333, 'completion_tokens': 24, 'total_tokens': 357}\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    status_code,\n",
    "    response,\n",
    "    rate_limit_info,\n",
    ") = call_azure_openai_chat_completions_api_with_pre_check(\n",
    "    deployment_id=azure_openai_client.completion_model_name,\n",
    "    method=\"post\",\n",
    "    body=body,\n",
    "    api_version=\"2023-05-15\",\n",
    ")\n",
    "\n",
    "# Print the status code, response, and rate limit info\n",
    "print(\"Status Code:\", status_code)\n",
    "print(\"Response:\", response)\n",
    "print(\"Rate Limit Info:\", rate_limit_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector-indexing-azureaisearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
