{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Prerequisites\n",
    "\n",
    "Ensure that your Azure Services are properly set up, your Conda environment is created, and your environment variables are configured as per the instructions in the [SETTINGS.md](SETTINGS.md) file.\n",
    "\n",
    "## ðŸ“‹ Table of Contents\n",
    "\n",
    "This notebook assists in testing and retrieving the headers of Azure OpenAI, covering the following sections:\n",
    "\n",
    "1. [**Setting Up Azure OpenAI Client**](#setting-up-azure-openai-client): Outlines the process of initializing the Azure OpenAI client.\n",
    "\n",
    "2. [**Calling Azure OpenAI API**](#calling-azure-openai-api): Discusses how to make API calls to Azure OpenAI.\n",
    "\n",
    "3. [**Extracting Headers and Payload Metadata**](#extracting-headers-and-payload-metadata): Explores how to extract headers and payload metadata from the API response.\n",
    "\n",
    "4. [**Analyzing Rate Limit Info**](#analyzing-rate-limit-info): Details the steps to analyze the rate limit information from the API response.\n",
    "\n",
    "For additional information, refer to the following resources:\n",
    "- [AOAI API Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\gbbai-azure-aoai-faq\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = (\n",
    "    r\"C:\\Users\\pablosal\\Desktop\\gbbai-azure-aoai-faq\"  # change your directory here\n",
    ")\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from dotenv import load_dotenv\n",
    "from src.aoai.azure_openai import AzureOpenAIManager\n",
    "from typing import Dict, Optional\n",
    "from requests import Response\n",
    "import requests\n",
    "\n",
    "from utils.ml_logging import get_logger\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Set up logger\n",
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Azure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the client. You can find it in src/aoai/azure_openai.py.\n",
    "# It is essentially a wrapper using dependency injection to automate the initialization\n",
    "# and most used API calls.\n",
    "azure_openai_client = AzureOpenAIManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling Azure OpenAI API\n",
    "\n",
    "### Creating Helper Function\n",
    "This function is designed to extract and build the metadata in JSON format. It's particularly useful for extracting rate limit and usage information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rate_limit_and_usage_info(response: Response) -> Dict[str, Optional[int]]:\n",
    "    \"\"\"\n",
    "    Extracts rate limiting information from the Azure Open API response headers and usage information from the payload.\n",
    "\n",
    "    :param response: The response object returned by a requests call.\n",
    "    :return: A dictionary containing the remaining requests, remaining tokens, and usage information\n",
    "             including prompt tokens, completion tokens, and total tokens.\n",
    "    \"\"\"\n",
    "    headers = response.headers\n",
    "    usage = response.json().get(\"usage\", {})\n",
    "    return {\n",
    "        \"remaining-requests\": headers.get(\"x-ratelimit-remaining-requests\"),\n",
    "        \"remaining-tokens\": headers.get(\"x-ratelimit-remaining-tokens\"),\n",
    "        \"prompt-tokens\": usage.get(\"prompt_tokens\"),\n",
    "        \"completion_tokens\": usage.get(\"completion_tokens\"),\n",
    "        \"total_tokens\": usage.get(\"total_tokens\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encapsulating Completions API Calls into a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_azure_openai_chat_completions_api(\n",
    "    deployment_id: str, method: str, body: dict = None, api_version: str = \"2023-11-01\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the Azure OpenAI API with the given parameters.\n",
    "\n",
    "    :param deployment_id: The ID of the deployment to access.\n",
    "    :param method: The HTTP method to use (\"get\" or \"post\").\n",
    "    :param body: The body of the request for \"post\" method. Defaults to None.\n",
    "    :param api_version: The API version to use. Defaults to \"2023-11-01\".\n",
    "\n",
    "    :return: The status code and response from the API call, along with rate limit headers.\n",
    "    \"\"\"\n",
    "    if method.lower() not in [\"get\", \"post\"]:\n",
    "        logger.error(\"Invalid HTTP method. Expected 'get' or 'post'.\")\n",
    "        return None, None, {}\n",
    "\n",
    "    url = f\"{azure_openai_client.azure_endpoint}/openai/deployments/{deployment_id}/chat/completions?api-version={api_version}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": azure_openai_client.api_key,\n",
    "        \"x-ms-useragent\": \"aoai-benchmark\"\n",
    "    }\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update(headers)\n",
    "\n",
    "        try:\n",
    "            if method.lower() == \"get\":\n",
    "                response = session.get(url)\n",
    "            else:  # method.lower() == \"post\"\n",
    "                response = session.post(url, json=body)\n",
    "            response.raise_for_status()  # Raises HTTPError for bad responses\n",
    "        except requests.HTTPError as http_err:\n",
    "            logger.error(f\"HTTP error occurred: {http_err}\")\n",
    "            return response.status_code, http_err.response.json(), {}\n",
    "        except Exception as err:\n",
    "            logger.error(f\"An error occurred: {err}\")\n",
    "            return None, None, {}\n",
    "\n",
    "    # Extract rate limit headers and usage details\n",
    "    rate_limit_headers = extract_rate_limit_and_usage_info(response)\n",
    "    print(response.headers)\n",
    "    return response.status_code, response.json(), rate_limit_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Headers and Payload Metadata\n",
    "\n",
    "#### Constructing the Request\n",
    "\n",
    "- **max_tokens**: Optional. Integer specifying the maximum number of tokens to generate. Default is 24.\n",
    "- **temperature**: Optional. Number between 0 and 2 indicating the sampling temperature. Default is 1.\n",
    "- **top_p**: Optional. Nucleus sampling parameter as a number between 0 and 1. Default is 1.\n",
    "- **user**: Optional. A unique identifier for the end-user to help monitor and detect abuse.\n",
    "- **n**: Optional. Integer for the number of completions to generate for each prompt. Default is 1.\n",
    "- **presence_penalty**: Optional. Number between -2.0 and 2.0 to penalize new tokens based on presence in the text so far. Default is 0.\n",
    "- **frequency_penalty**: Optional. Number between -2.0 and 2.0 to penalize new tokens based on frequency in the text so far. Default is 0.\n",
    "- **messages**: Optional. An array of message objects.\n",
    "\n",
    "You can learn more about the aoai API  [official documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "    \"max_tokens\": 24,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 1,\n",
    "    \"user\": \"\",\n",
    "    \"n\": 1,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Does Azure OpenAI support customer managed keys?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Yes, customer managed keys are supported by Azure OpenAI.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Do other Azure AI services support this too?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Yes, other Azure AI services also support customer managed keys.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Can you tell me more about these services?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Sure, Azure AI services include Azure Cognitive Services, Azure Machine Learning, and more.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is Azure Cognitive Services?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Azure Cognitive Services is a collection of APIs and services for building intelligent applications.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is Azure Machine Learning?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Azure Machine Learning is a cloud-based service for building, training, and deploying machine learning models.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Thank you for the information.\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"You're welcome! If you have any other questions, feel free to ask.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What other services does Azure offer?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Azure offers a wide range of services including computing, analytics, storage, and networking.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you tell me more about Azure's computing services?\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Azure's computing services include virtual machines, container services, and serverless computing.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is serverless computing?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Serverless computing is a cloud computing model where the cloud provider automatically manages the provisioning and scaling of servers.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"That's interesting. Thank you for the information.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"You're welcome! If you have any other questions, feel free to ask.\",\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cache-Control': 'no-cache, must-revalidate', 'Content-Length': '428', 'Content-Type': 'application/json', 'access-control-allow-origin': '*', 'apim-request-id': 'a4c6189e-8b0f-4a49-bf03-343bfe7c87f8', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ms-region': 'Canada East', 'x-ratelimit-remaining-requests': '9', 'x-ratelimit-remaining-tokens': '9629', 'x-accel-buffering': 'no', 'x-request-id': '6e7347f6-69db-480e-bf5b-934cf5130d7b', 'x-ms-client-request-id': 'a4c6189e-8b0f-4a49-bf03-343bfe7c87f8', 'azureml-model-session': 'd018-20240201032837', 'Date': 'Fri, 02 Feb 2024 05:13:43 GMT'}\n",
      "Status Code: 200\n",
      "Response: {'id': 'chatcmpl-8ng9sFt76UntMD7xLTfd29Ql4dbJA', 'object': 'chat.completion', 'created': 1706850820, 'model': 'gpt-4', 'choices': [{'finish_reason': 'length', 'index': 0, 'message': {'role': 'assistant', 'content': \"You're welcome! If you have any more questions or need further clarification on any topic, feel free to reach out.\"}}], 'usage': {'prompt_tokens': 333, 'completion_tokens': 24, 'total_tokens': 357}, 'system_fingerprint': 'fp_68a7d165bf'}\n",
      "Rate Limit Info: {'remaining-requests': '9', 'remaining-tokens': '9629', 'prompt-tokens': 333, 'completion_tokens': 24, 'total_tokens': 357}\n"
     ]
    }
   ],
   "source": [
    "status_code, response, rate_limit_info = call_azure_openai_chat_completions_api(\n",
    "    deployment_id=azure_openai_client.chat_model_name,\n",
    "    method=\"post\",\n",
    "    body=body,\n",
    "    api_version=\"2023-05-15\",\n",
    ")\n",
    "\n",
    "# Print the status code, response, and rate limit info\n",
    "print(\"Status Code:\", status_code)\n",
    "print(\"Response:\", response)\n",
    "print(\"Rate Limit Info:\", rate_limit_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Rate Limit Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rate limit information provides details about the usage of the API:\n",
    "\n",
    "- **Remaining Requests**: The number of API calls that can still be made. In this case, there are 9 requests left.\n",
    "- **Remaining Tokens**: The number of tokens that can still be generated. In this case, there are 9258 tokens left.\n",
    "- **Prompt Tokens**: The number of tokens used in the prompt for this API call. In this case, 333 tokens were used.\n",
    "- **Completion Tokens**: The number of tokens generated in the completion for this API call. In this case, 24 tokens were generated.\n",
    "- **Total Tokens**: The total number of tokens used in this API call. This is the sum of the prompt tokens and the completion tokens. In this case, 357 tokens were used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Programmatically Stop Call if Input Tokens Exceed Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RETRY_SECONDS = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, Any, Optional, Union\n",
    "import backoff\n",
    "import time\n",
    "import logging\n",
    "my_logger = logging.getLogger('my_logger')\n",
    "my_handler = logging.StreamHandler()\n",
    "my_logger.addHandler(my_handler)\n",
    "my_logger.setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _terminal_http_code(e) -> bool:\n",
    "    \"\"\"\n",
    "    Determine whether to give up retrying based on the HTTP status code.\n",
    "    \"\"\"\n",
    "    # Assuming e is an exception instance from the requests library.\n",
    "    status_code = e.response.status_code\n",
    "    if status_code == 429:\n",
    "        logger.info(f\"429 status code received, will retry. Error: {e}\")\n",
    "    else:\n",
    "        logger.info(f\"Non-retryable status code {status_code} received. Error: {e}\")\n",
    "    return status_code != 429\n",
    "\n",
    "def backoff_hdlr(details):\n",
    "    \"\"\"\n",
    "    Log details when a backoff occurs.\n",
    "    \"\"\"\n",
    "    logger.warning(f\"Backing off {details['wait']:0.1f} seconds after {details['tries']} tries calling function {details['target']} with args {details['args']} and kwargs {details['kwargs']}\")\n",
    "\n",
    "def giveup_hdlr(details):\n",
    "    \"\"\"\n",
    "    Log details when giving up on retries.\n",
    "    \"\"\"\n",
    "    logger.warning(f\"Giving up after {details['tries']} tries calling function {details['target']} with args {details['args']} and kwargs {details['kwargs']}\")\n",
    "\n",
    "@backoff.on_exception(backoff.expo,\n",
    "                    requests.exceptions.RequestException,\n",
    "                    jitter=backoff.full_jitter,\n",
    "                    max_time=MAX_RETRY_SECONDS,\n",
    "                    giveup=_terminal_http_code,\n",
    "                    max_tries=5,\n",
    "                    on_backoff=backoff_hdlr,\n",
    "                    on_giveup=giveup_hdlr,\n",
    "                    logger=my_logger)\n",
    "def call_azure_openai_chat_completions_api_with_pre_check(\n",
    "    deployment_id: str,\n",
    "    method: str,\n",
    "    body: Optional[Dict[str, Any]] = None,\n",
    "    api_version: str = \"2023-11-01\",\n",
    ") -> Tuple[Optional[int], Optional[Union[Dict[str, Any], str]], Dict[str, str]]:\n",
    "    if method.lower() not in [\"get\", \"post\"]:\n",
    "        logger.error(\"Invalid HTTP method. Expected 'get' or 'post'.\")\n",
    "        return None, \"Invalid HTTP method. Expected 'get' or 'post'.\", {}\n",
    "\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < MAX_RETRY_SECONDS:\n",
    "        (\n",
    "            status_code,\n",
    "            response,\n",
    "            rate_limit_headers,\n",
    "        ) = call_azure_openai_chat_completions_api(\n",
    "            deployment_id, method=method, body=body, api_version=api_version)\n",
    "        if status_code != 429:\n",
    "            break\n",
    "        time.sleep(1)  # sleep for a while before retrying\n",
    "\n",
    "    return status_code, response, rate_limit_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def call_api():\n",
    "    status_code, response, rate_limit_info = call_azure_openai_chat_completions_api_with_pre_check(\n",
    "        deployment_id=azure_openai_client.chat_model_name,\n",
    "        method=\"post\",\n",
    "        body=body,\n",
    "        api_version=\"2023-05-15\",\n",
    "    )\n",
    "    return status_code, response, rate_limit_info\n",
    "\n",
    "async def main():\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        tasks = []\n",
    "        for _ in range(10):\n",
    "            tasks.append(loop.run_in_executor(executor, call_api))\n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "\n",
    "        for response in responses:\n",
    "            print(response)\n",
    "\n",
    "# Run the main function\n",
    "for _ in range(10):\n",
    "    await main()\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector-indexing-azureaisearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
