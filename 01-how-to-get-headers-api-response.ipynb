{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Prerequisites\n",
    "\n",
    "Ensure that your Azure Services are properly set up, your Conda environment is created, and your environment variables are configured as per the instructions in the [SETTINGS.md](SETTINGS.md) file.\n",
    "\n",
    "## ðŸ“‹ Table of Contents\n",
    "\n",
    "This notebook assists in testing and retrieving the headers of Azure OpenAI, covering the following sections:\n",
    "\n",
    "1. [**Setting Up Azure OpenAI Client**](#setting-up-azure-openai-client): Outlines the process of initializing the Azure OpenAI client.\n",
    "\n",
    "2. [**Calling Azure OpenAI API**](#calling-azure-openai-api): Discusses how to make API calls to Azure OpenAI.\n",
    "\n",
    "3. [**Extracting Headers and Payload Metadata**](#extracting-headers-and-payload-metadata): Explores how to extract headers and payload metadata from the API response.\n",
    "\n",
    "4. [**Analyzing Rate Limit Info**](#analyzing-rate-limit-info): Details the steps to analyze the rate limit information from the API response.\n",
    "\n",
    "For additional information, refer to the following resources:\n",
    "- [AOAI API Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference)\n",
    "\n",
    "And visit the blog here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\gbbai-azure-aoai-faq\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = (\n",
    "    r\"C:\\Users\\pablosal\\Desktop\\gbbai-azure-aoai-faq\"  # change your directory here\n",
    ")\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from dotenv import load_dotenv\n",
    "from src.aoai.azure_openai import AzureOpenAIManager\n",
    "from typing import Dict, Optional\n",
    "from requests import Response\n",
    "import requests\n",
    "\n",
    "from utils.ml_logging import get_logger\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Set up logger\n",
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Azure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the client. You can find it in src/aoai/azure_openai.py.\n",
    "# It is essentially a wrapper using dependency injection to automate the initialization\n",
    "# and most used API calls.\n",
    "azure_openai_client = AzureOpenAIManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling Azure OpenAI API\n",
    "\n",
    "### Creating Helper Function\n",
    "This function is designed to extract and build the metadata in JSON format. It's particularly useful for extracting rate limit and usage information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rate_limit_and_usage_info(response: Response) -> Dict[str, Optional[int]]:\n",
    "    \"\"\"\n",
    "    Extracts rate limiting information from the Azure Open API response headers and usage information from the payload.\n",
    "\n",
    "    :param response: The response object returned by a requests call.\n",
    "    :return: A dictionary containing the remaining requests, remaining tokens, and usage information\n",
    "             including prompt tokens, completion tokens, and total tokens.\n",
    "    \"\"\"\n",
    "    headers = response.headers\n",
    "    usage = response.json().get(\"usage\", {})\n",
    "    return {\n",
    "        \"remaining-requests\": headers.get(\"x-ratelimit-remaining-requests\"),\n",
    "        \"remaining-tokens\": headers.get(\"x-ratelimit-remaining-tokens\"),\n",
    "        \"retry-after\": headers.get(\"retry-after\", None), \n",
    "        \"prompt-tokens\": usage.get(\"prompt_tokens\", None),\n",
    "        \"completion_tokens\": usage.get(\"completion_tokens\", None),\n",
    "        \"total_tokens\": usage.get(\"total_tokens\", None),\n",
    "     \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encapsulating Completions API Calls into a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_azure_openai_chat_completions_api(\n",
    "    deployment_id: str, body: dict = None, api_version: str = \"2023-11-01\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the Azure OpenAI API with the given parameters.\n",
    "\n",
    "    :param deployment_id: The ID of the deployment to access.\n",
    "    :param body: The body of the request for \"post\" method. Defaults to None.\n",
    "    :param api_version: The API version to use. Defaults to \"2023-11-01\".\n",
    "\n",
    "    :return: The status code and response from the API call, along with rate limit headers.\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"{azure_openai_client.azure_endpoint}/openai/deployments/{deployment_id}/chat/completions?api-version={api_version}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": azure_openai_client.api_key,\n",
    "        \"x-ms-useragent\": \"aoai-faq\"\n",
    "    }\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update(headers)\n",
    "\n",
    "    try:\n",
    "        response = session.post(url, json=body)\n",
    "        response.raise_for_status()  # Raises HTTPError for bad responses\n",
    "    except requests.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err}\")\n",
    "        print(f\"HEADERS -> {response.headers}\")\n",
    "        return response.status_code, http_err.response.json(), {}\n",
    "    except Exception as err:\n",
    "        logger.error(f\"An error occurred: {err}\")\n",
    "        print(f\"HEADERS -> {response.headers}\")\n",
    "        return None, None, {}\n",
    "\n",
    "    # Extract rate limit headers and usage details\n",
    "    rate_limit_headers = extract_rate_limit_and_usage_info(response)\n",
    "    print(f\"HEADERS -> {response.headers}\")\n",
    "    print(f\"STATUS CODE -> {response.status_code}\")\n",
    "    return response.status_code, response.json(), rate_limit_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Headers and Payload Metadata\n",
    "\n",
    "#### Constructing the Request\n",
    "\n",
    "- **max_tokens**: Optional. Integer specifying the maximum number of tokens to generate. Default is 24.\n",
    "- **temperature**: Optional. Number between 0 and 2 indicating the sampling temperature. Default is 1.\n",
    "- **top_p**: Optional. Nucleus sampling parameter as a number between 0 and 1. Default is 1.\n",
    "- **user**: Optional. A unique identifier for the end-user to help monitor and detect abuse.\n",
    "- **n**: Optional. Integer for the number of completions to generate for each prompt. Default is 1.\n",
    "- **presence_penalty**: Optional. Number between -2.0 and 2.0 to penalize new tokens based on presence in the text so far. Default is 0.\n",
    "- **frequency_penalty**: Optional. Number between -2.0 and 2.0 to penalize new tokens based on frequency in the text so far. Default is 0.\n",
    "- **messages**: Optional. An array of message objects.\n",
    "\n",
    "You can learn more about the aoai API  [official documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "    \"max_tokens\": 3000,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 1,\n",
    "    \"user\": \"\",\n",
    "    \"n\": 1,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Does Azure OpenAI support customer-managed keys?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": '''Yes, Azure OpenAI supports customer-managed keys, allowing customers \n",
    "            to control encryption keys and ensuring that data remains secure.'''\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Do other Azure AI services support this as well?\"},\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADERS -> {'Content-Length': '1658', 'Content-Type': 'application/json', 'x-ms-region': 'East US 2', 'apim-request-id': '23258c34-4768-4207-875e-4f03c5b33e7b', 'x-ratelimit-remaining-requests': '449', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': 'd044fe06-d994-4b72-9041-37a8711e8599', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'azureml-model-session': 'd065-20240607042906', 'x-content-type-options': 'nosniff', 'x-envoy-upstream-service-time': '4734', 'x-ms-client-request-id': '23258c34-4768-4207-875e-4f03c5b33e7b', 'x-ratelimit-remaining-tokens': '446929', 'Date': 'Fri, 02 Aug 2024 23:12:41 GMT'}\n",
      "STATUS CODE -> 200\n",
      "Status Code: 200\n",
      "Response: {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}, 'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Yes, several other Azure AI services also support customer-managed keys for enhanced security and control over encryption. These services include:\\n\\n1. **Azure Cognitive Services**: Many of the individual services within Azure Cognitive Services, such as Text Analytics, Computer Vision, and more, support customer-managed keys.\\n\\n2. **Azure Machine Learning**: This service supports customer-managed keys to encrypt data stored in Azure Blob Storage and Azure SQL Database, among other storage options.\\n\\n3. **Azure Synapse Analytics**: For AI and machine learning workloads, Azure Synapse Analytics also supports customer-managed keys for encrypting data.\\n\\nUsing customer-managed keys allows organizations to manage their encryption keys through Azure Key Vault, giving them greater control over data security and compliance requirements.', 'role': 'assistant'}}], 'created': 1722640357, 'id': 'chatcmpl-9rvjpi29Lap5MaOuQ9IxK1Xg5t5KA', 'model': 'gpt-4o-2024-05-13', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'system_fingerprint': 'fp_abc28019ad', 'usage': {'completion_tokens': 147, 'prompt_tokens': 69, 'total_tokens': 216}}\n",
      "Rate Limit Info: {'remaining-requests': '449', 'remaining-tokens': '446929', 'retry-after': None, 'prompt-tokens': 69, 'completion_tokens': 147, 'total_tokens': 216}\n"
     ]
    }
   ],
   "source": [
    "status_code, response, rate_limit_info = call_azure_openai_chat_completions_api(\n",
    "    deployment_id=azure_openai_client.chat_model_name,\n",
    "    body=body,\n",
    "    api_version=\"2024-02-15-preview\",\n",
    ")\n",
    "\n",
    "# Print the status code, response, and rate limit info\n",
    "print(\"Status Code:\", status_code)\n",
    "print(\"Response:\", response)\n",
    "print(\"Rate Limit Info:\", rate_limit_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate Limit Info: {'remaining-requests': '449', 'remaining-tokens': '446929', 'retry-after': None, 'prompt-tokens': 69, 'completion_tokens': 147, 'total_tokens': 216}\n"
     ]
    }
   ],
   "source": [
    "print(\"Rate Limit Info:\", rate_limit_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Remaining Requests**: The number of API calls that can still be made. In this case, there are 449 requests left.\n",
    "- **Remaining Tokens**: The number of tokens that can still be generated. In this case, there are 443,538 tokens left.\n",
    "- **Prompt Tokens**: The number of tokens used in the prompt for this API call. In this case, 192 tokens were used.\n",
    "- **Completion Tokens**: The number of tokens generated in the completion for this API call. In this case, 938 tokens were generated.\n",
    "- **Total Tokens**: The total number of tokens used in this API call. This is the sum of the prompt tokens and the completion tokens. In this case, 1,130 tokens were used.\n",
    "- **Retry-After**: The time to wait before making another request. In this case, it is not specified (None)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Current Rate Limitations Before Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ml_logging import get_logger\n",
    "from src.aoai.azure_openai import AzureOpenAIManager\n",
    "from src.aoai.tokenizer import AzureOpenAITokenizer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Set up logger\n",
    "logger = get_logger()\n",
    "\n",
    "azure_openai_client = AzureOpenAIManager(\n",
    "    api_key=os.getenv('AZURE_OPENAI_KEY_TEST'), \n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_API_ENDPOINT_TEST'), \n",
    "    chat_model_name=os.getenv('AZURE_AOAI_CHAT_MODEL_NAME_DEPLOYMENT_ID_TEST'), \n",
    "    api_version=\"2024-05-13\"\n",
    ")\n",
    "tokenizer = AzureOpenAITokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "from requests import Response\n",
    "from threading import Lock\n",
    "\n",
    "# Global variables to store current rate limits\n",
    "# Fake global values obtained from previous calls\n",
    "current_rate_limits = {\n",
    "    \"remaining_requests\": 5,  \n",
    "    \"remaining_tokens\": 300\n",
    "}\n",
    "\n",
    "rate_limit_lock = Lock()  # To safely update rate limits across threads\n",
    "\n",
    "def update_rate_limits(headers: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Update the global rate limit tracker based on the headers from the response.\n",
    "    \"\"\"\n",
    "    global current_rate_limits\n",
    "    with rate_limit_lock:\n",
    "        current_rate_limits[\"remaining_requests\"] = int(headers.get(\"x-ratelimit-remaining-requests\", 0))\n",
    "        current_rate_limits[\"remaining_tokens\"] = int(headers.get(\"x-ratelimit-remaining-tokens\", 0))\n",
    "        logger.info(f\"Updated rate limits: {current_rate_limits}\")\n",
    "\n",
    "def extract_rate_limit_and_usage_info(response: Response) -> Dict[str, Optional[int]]:\n",
    "    \"\"\"\n",
    "    Extracts rate limiting information from the Azure Open AI response headers and usage information from the payload.\n",
    "\n",
    "    :param response: The response object returned by a requests call.\n",
    "    :return: A dictionary containing the remaining requests, remaining tokens, and usage information.\n",
    "    \"\"\"\n",
    "    headers = response.headers\n",
    "    usage = response.json().get(\"usage\", {})\n",
    "    update_rate_limits(headers)\n",
    "    return {\n",
    "        \"remaining-requests\": int(headers.get(\"x-ratelimit-remaining-requests\", 0)),\n",
    "        \"remaining-tokens\": int(headers.get(\"x-ratelimit-remaining-tokens\", 0)),\n",
    "        \"retry-after\": headers.get(\"retry-after\"),\n",
    "        \"prompt-tokens\": usage.get(\"prompt_tokens\"),\n",
    "        \"completion-tokens\": usage.get(\"completion_tokens\"),\n",
    "        \"total-tokens\": usage.get(\"total_tokens\"),\n",
    "    }\n",
    "\n",
    "def call_azure_openai_chat_completions_api(\n",
    "    deployment_id: str, body: Dict, api_version: str = \"2023-11-01\"\n",
    ") -> Tuple[Optional[int], Optional[Dict], Optional[Dict[str, Optional[int]]]]:\n",
    "    \"\"\"\n",
    "    Calls the Azure OpenAI API with the given parameters.\n",
    "\n",
    "    :param deployment_id: The ID of the deployment to access.\n",
    "    :param body: The body of the request for \"post\" method. Defaults to None.\n",
    "    :param api_version: The API version to use. Defaults to \"2023-11-01\".\n",
    "\n",
    "    :return: The status code and response from the API call, along with rate limit headers.\n",
    "    \"\"\"\n",
    "    url = f\"{azure_openai_client.azure_endpoint}/openai/deployments/{deployment_id}/chat/completions?api-version={api_version}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": azure_openai_client.api_key,\n",
    "        \"x-ms-useragent\": \"aoai-faq\"\n",
    "    }\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update(headers)\n",
    "        response = session.post(url, json=body)\n",
    "        response.raise_for_status()\n",
    "\n",
    "    rate_limit_headers = extract_rate_limit_and_usage_info(response)\n",
    "    logger.debug(f\"Rate limit headers: {rate_limit_headers}\")\n",
    "    return response.status_code, response.json(), rate_limit_headers\n",
    "\n",
    "def can_make_request(body: Dict, model: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if the current request can be made based on the remaining rate limits.\n",
    "\n",
    "    :param body: The body of the request to be sent.\n",
    "    :param model: The model name used for estimating tokens.\n",
    "    :return: True if the request can be made, False otherwise.\n",
    "    \"\"\"\n",
    "    global current_rate_limits\n",
    "    required_tokens = tokenizer.estimate_tokens_azure_openai(messages=body[\"messages\"], model=model)\n",
    "\n",
    "    with rate_limit_lock:\n",
    "        logger.info(f\"Current rate limits before check: {current_rate_limits}\")\n",
    "        logger.info(f\"Required tokens for the request: {required_tokens}\")\n",
    "\n",
    "        can_make_request = True\n",
    "\n",
    "        if current_rate_limits[\"remaining_requests\"] is not None:\n",
    "            if current_rate_limits[\"remaining_requests\"] <= 0:\n",
    "                logger.info(\"Cannot make request: No remaining requests.\")\n",
    "                can_make_request = False\n",
    "            else:\n",
    "                logger.info(f\"Remaining requests: {current_rate_limits['remaining_requests']}\")\n",
    "\n",
    "        if current_rate_limits[\"remaining_tokens\"] is not None:\n",
    "            if current_rate_limits[\"remaining_tokens\"] < required_tokens:\n",
    "                logger.info(\"Cannot make request: Not enough remaining tokens.\")\n",
    "                can_make_request = False\n",
    "            else:\n",
    "                logger.debug(f\"Remaining tokens: {current_rate_limits['remaining_tokens']}\")\n",
    "\n",
    "        if can_make_request:\n",
    "            if current_rate_limits[\"remaining_requests\"] is not None:\n",
    "                current_rate_limits[\"remaining_requests\"] -= 1\n",
    "                logger.info(f\"Decremented remaining requests: {current_rate_limits['remaining_requests']}\")\n",
    "            if current_rate_limits[\"remaining_tokens\"] is not None:\n",
    "                current_rate_limits[\"remaining_tokens\"] -= required_tokens\n",
    "                logger.info(f\"Decremented remaining tokens: {current_rate_limits['remaining_tokens']}\")\n",
    "            logger.info(f\"After decrement, rate limits: {current_rate_limits}\")\n",
    "            return True\n",
    "\n",
    "    logger.info(\"Request cannot be made due to rate limits.\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 19:15:13,011 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 5, 'remaining_tokens': 300} (2257159956.py:can_make_request:85)\n",
      "2024-08-02 19:15:13,014 - micro - MainProcess - INFO     Required tokens for the request: 69 (2257159956.py:can_make_request:86)\n",
      "2024-08-02 19:15:13,015 - micro - MainProcess - INFO     Remaining requests: 5 (2257159956.py:can_make_request:95)\n",
      "2024-08-02 19:15:13,016 - micro - MainProcess - INFO     Decremented remaining requests: 4 (2257159956.py:can_make_request:107)\n",
      "2024-08-02 19:15:13,017 - micro - MainProcess - INFO     Decremented remaining tokens: 231 (2257159956.py:can_make_request:110)\n",
      "2024-08-02 19:15:13,017 - micro - MainProcess - INFO     After decrement, rate limits: {'remaining_requests': 4, 'remaining_tokens': 231} (2257159956.py:can_make_request:111)\n",
      "2024-08-02 19:15:15,390 - micro - MainProcess - INFO     Updated rate limits: {'remaining_requests': 9, 'remaining_tokens': 6932} (2257159956.py:update_rate_limits:24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}, 'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Yes, several other Azure AI services also support customer-managed keys (CMKs) to enhance security and give customers more control over their data. These services include but are not limited to:\\n\\n1. **Azure Cognitive Services:** Many of the individual services within Azure Cognitive Services support customer-managed keys. This allows you to control the encryption of your data processed by these services.\\n   \\n2. **Azure Machine Learning:** Azure Machine Learning supports customer-managed keys for the encryption of data stored in the associated storage accounts.\\n\\n3. **Azure Synapse Analytics:** This service also supports customer-managed keys, giving you control over the encryption of your data stored in Synapse workspaces.\\n\\n4. **Azure Search (now known as Azure Cognitive Search):** Customer-managed keys can be used to encrypt data at rest in Azure Cognitive Search.\\n\\nThese features help ensure that organizations can meet compliance requirements and maintain a higher level of security by controlling the encryption keys used to protect their data.', 'role': 'assistant'}}], 'created': 1722644113, 'id': 'chatcmpl-9rwiPJ4Sug6MD7LbSTWUmni6ybgmW', 'model': 'gpt-4o-2024-05-13', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'system_fingerprint': 'fp_abc28019ad', 'usage': {'completion_tokens': 190, 'prompt_tokens': 67, 'total_tokens': 257}}\n",
      "Rate Limit Info: {'remaining-requests': 9, 'remaining-tokens': 6932, 'retry-after': None, 'prompt-tokens': 67, 'completion-tokens': 190, 'total-tokens': 257}\n"
     ]
    }
   ],
   "source": [
    "if can_make_request(body, \"gpt-4-0613\"):\n",
    "    status_code, response, rate_limit_info = call_azure_openai_chat_completions_api(\n",
    "        deployment_id=azure_openai_client.chat_model_name,\n",
    "        body=body,\n",
    "        api_version=\"2024-02-15-preview\",\n",
    "    )\n",
    "    print(\"Status Code:\", status_code)\n",
    "    print(\"Response:\", response)\n",
    "    print(\"Rate Limit Info:\", rate_limit_info)\n",
    "else:\n",
    "    print(\"Rate limit exceeded, cannot make the request.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector-indexing-azureaisearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
