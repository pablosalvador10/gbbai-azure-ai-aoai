{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Prerequisites\n",
    "\n",
    "Ensure that your Azure Services are properly set up, your Conda environment is created, and your environment variables are configured as per the instructions in the [SETTINGS.md](SETTINGS.md) file.\n",
    "\n",
    "## ðŸ“‹ Table of Contents\n",
    "\n",
    "This notebook assists in testing and retrieving the headers of Azure OpenAI, covering the following sections:\n",
    "\n",
    "1. [**Setting Up Azure OpenAI Client**](#setting-up-azure-openai-client): Outlines the process of initializing the Azure OpenAI client.\n",
    "\n",
    "2. [**Calling Azure OpenAI API**](#calling-azure-openai-api): Discusses how to make API calls to Azure OpenAI.\n",
    "\n",
    "3. [**Extracting Headers and Payload Metadata**](#extracting-headers-and-payload-metadata): Explores how to extract headers and payload metadata from the API response.\n",
    "\n",
    "4. [**Analyzing Rate Limit Info**](#analyzing-rate-limit-info): Details the steps to analyze the rate limit information from the API response.\n",
    "\n",
    "For additional information, refer to the following resources:\n",
    "- [Visit the blog for deep insights](https://pabloaicorner.hashnode.dev/how-to-extract-and-analyze-azure-openai-response-headers)\n",
    "- [AOAI API Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\gbbai-azure-aoai-faq\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = (\n",
    "    r\"C:\\Users\\pablosal\\Desktop\\gbbai-azure-aoai-faq\"  # change your directory here\n",
    ")\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from dotenv import load_dotenv\n",
    "from src.aoai.azure_openai import AzureOpenAIManager\n",
    "from typing import Dict, Optional\n",
    "from requests import Response\n",
    "import requests\n",
    "\n",
    "from utils.ml_logging import get_logger\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Set up logger\n",
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Azure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the client. You can find it in src/aoai/azure_openai.py.\n",
    "# It is essentially a wrapper using dependency injection to automate the initialization\n",
    "# and most used API calls.\n",
    "azure_openai_client = AzureOpenAIManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling Azure OpenAI API\n",
    "\n",
    "### Creating Helper Function\n",
    "This function is designed to extract and build the metadata in JSON format. It's particularly useful for extracting rate limit and usage information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rate_limit_and_usage_info(response: Response) -> Dict[str, Optional[int]]:\n",
    "    \"\"\"\n",
    "    Extracts rate limiting information from the Azure Open API response headers and usage information from the payload.\n",
    "\n",
    "    :param response: The response object returned by a requests call.\n",
    "    :return: A dictionary containing the remaining requests, remaining tokens, and usage information\n",
    "             including prompt tokens, completion tokens, and total tokens.\n",
    "    \"\"\"\n",
    "    headers = response.headers\n",
    "    usage = response.json().get(\"usage\", {})\n",
    "    return {\n",
    "        \"remaining-requests\": headers.get(\"x-ratelimit-remaining-requests\"),\n",
    "        \"remaining-tokens\": headers.get(\"x-ratelimit-remaining-tokens\"),\n",
    "        \"retry-after\": headers.get(\"retry-after\", None), \n",
    "        \"prompt-tokens\": usage.get(\"prompt_tokens\", None),\n",
    "        \"completion_tokens\": usage.get(\"completion_tokens\", None),\n",
    "        \"total_tokens\": usage.get(\"total_tokens\", None),\n",
    "     \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encapsulating Completions API Calls into a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_azure_openai_chat_completions_api(\n",
    "    deployment_id: str, body: dict = None, api_version: str = \"2023-11-01\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the Azure OpenAI API with the given parameters.\n",
    "\n",
    "    :param deployment_id: The ID of the deployment to access.\n",
    "    :param body: The body of the request for \"post\" method. Defaults to None.\n",
    "    :param api_version: The API version to use. Defaults to \"2023-11-01\".\n",
    "\n",
    "    :return: The status code and response from the API call, along with rate limit headers.\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"{azure_openai_client.azure_endpoint}/openai/deployments/{deployment_id}/chat/completions?api-version={api_version}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": azure_openai_client.api_key,\n",
    "        \"x-ms-useragent\": \"aoai-faq\"\n",
    "    }\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update(headers)\n",
    "\n",
    "    try:\n",
    "        response = session.post(url, json=body)\n",
    "        response.raise_for_status()  # Raises HTTPError for bad responses\n",
    "    except requests.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err}\")\n",
    "        print(f\"HEADERS -> {response.headers}\")\n",
    "        return response.status_code, http_err.response.json(), {}\n",
    "    except Exception as err:\n",
    "        logger.error(f\"An error occurred: {err}\")\n",
    "        print(f\"HEADERS -> {response.headers}\")\n",
    "        return None, None, {}\n",
    "\n",
    "    # Extract rate limit headers and usage details\n",
    "    rate_limit_headers = extract_rate_limit_and_usage_info(response)\n",
    "    print(f\"HEADERS -> {response.headers}\")\n",
    "    print(f\"STATUS CODE -> {response.status_code}\")\n",
    "    return response.status_code, response.json(), rate_limit_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Headers and Payload Metadata\n",
    "\n",
    "#### Constructing the Request\n",
    "\n",
    "- **max_tokens**: Optional. Integer specifying the maximum number of tokens to generate. Default is 24.\n",
    "- **temperature**: Optional. Number between 0 and 2 indicating the sampling temperature. Default is 1.\n",
    "- **top_p**: Optional. Nucleus sampling parameter as a number between 0 and 1. Default is 1.\n",
    "- **user**: Optional. A unique identifier for the end-user to help monitor and detect abuse.\n",
    "- **n**: Optional. Integer for the number of completions to generate for each prompt. Default is 1.\n",
    "- **presence_penalty**: Optional. Number between -2.0 and 2.0 to penalize new tokens based on presence in the text so far. Default is 0.\n",
    "- **frequency_penalty**: Optional. Number between -2.0 and 2.0 to penalize new tokens based on frequency in the text so far. Default is 0.\n",
    "- **messages**: Optional. An array of message objects.\n",
    "\n",
    "You can learn more about the aoai API  [official documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "    \"max_tokens\": 3000,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 1,\n",
    "    \"user\": \"\",\n",
    "    \"n\": 1,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Does Azure OpenAI support customer-managed keys?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": '''Yes, Azure OpenAI supports customer-managed keys, allowing customers \n",
    "            to control encryption keys and ensuring that data remains secure.'''\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Do other Azure AI services support this as well?\"},\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADERS -> {'Content-Length': '1664', 'Content-Type': 'application/json', 'x-ms-region': 'East US 2', 'apim-request-id': 'd1a8168e-525a-4534-8e55-70acd2ba2915', 'x-ratelimit-remaining-requests': '449', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': 'd3320dcf-c14a-4d9a-9a83-fa7cd8db52ea', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'azureml-model-session': 'd099-20240615084832', 'x-content-type-options': 'nosniff', 'x-envoy-upstream-service-time': '2291', 'x-ms-client-request-id': 'd1a8168e-525a-4534-8e55-70acd2ba2915', 'x-ratelimit-remaining-tokens': '446929', 'Date': 'Sun, 04 Aug 2024 16:11:25 GMT'}\n",
      "STATUS CODE -> 200\n",
      "Status Code: 200\n",
      "Response: {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}, 'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Yes, several other Azure AI services also support customer-managed keys, enabling additional control over data encryption. Some of these services include:\\n\\n1. **Azure Cognitive Services**: Various services under this umbrella, such as Text Analytics, Computer Vision, and Speech Services, support customer-managed keys.\\n\\n2. **Azure Machine Learning**: This service allows you to use customer-managed keys to encrypt data stored in your workspace.\\n\\n3. **Azure Synapse Analytics**: This service supports customer-managed keys for data encryption in your Synapse workspace.\\n\\n4. **Form Recognizer**: This is another Cognitive Service that supports customer-managed keys for encrypting the data being processed.\\n\\nAlways refer to the official Azure documentation for the most up-to-date and specific information on supported features and services.', 'role': 'assistant'}}], 'created': 1722787884, 'id': 'chatcmpl-9sY7IprSPmIUF9Zb9fzZBswgzDtds', 'model': 'gpt-4o-2024-05-13', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'system_fingerprint': 'fp_abc28019ad', 'usage': {'completion_tokens': 154, 'prompt_tokens': 69, 'total_tokens': 223}}\n",
      "Rate Limit Info: {'remaining-requests': '449', 'remaining-tokens': '446929', 'retry-after': None, 'prompt-tokens': 69, 'completion_tokens': 154, 'total_tokens': 223}\n"
     ]
    }
   ],
   "source": [
    "status_code, response, rate_limit_info = call_azure_openai_chat_completions_api(\n",
    "    deployment_id=azure_openai_client.chat_model_name,\n",
    "    body=body,\n",
    "    api_version=\"2024-02-15-preview\",\n",
    ")\n",
    "\n",
    "# Print the status code, response, and rate limit info\n",
    "print(\"Status Code:\", status_code)\n",
    "print(\"Response:\", response)\n",
    "print(\"Rate Limit Info:\", rate_limit_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate Limit Info: {'remaining-requests': '449', 'remaining-tokens': '446929', 'retry-after': None, 'prompt-tokens': 69, 'completion_tokens': 154, 'total_tokens': 223}\n"
     ]
    }
   ],
   "source": [
    "print(\"Rate Limit Info:\", rate_limit_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Remaining Requests**: The number of API calls that can still be made. In this case, there are 449 requests left.\n",
    "- **Remaining Tokens**: The number of tokens that can still be generated. In this case, there are 443,538 tokens left.\n",
    "- **Prompt Tokens**: The number of tokens used in the prompt for this API call. In this case, 192 tokens were used.\n",
    "- **Completion Tokens**: The number of tokens generated in the completion for this API call. In this case, 938 tokens were generated.\n",
    "- **Total Tokens**: The total number of tokens used in this API call. This is the sum of the prompt tokens and the completion tokens. In this case, 1,130 tokens were used.\n",
    "- **Retry-After**: The time to wait before making another request. In this case, it is not specified (None)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Current Rate Limitations Before Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ml_logging import get_logger\n",
    "from src.aoai.azure_openai import AzureOpenAIManager\n",
    "from src.aoai.tokenizer import AzureOpenAITokenizer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Set up logger\n",
    "logger = get_logger()\n",
    "\n",
    "azure_openai_client = AzureOpenAIManager(\n",
    "    api_key=os.getenv('AZURE_OPENAI_KEY_TEST'), \n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_API_ENDPOINT_TEST'), \n",
    "    chat_model_name=os.getenv('AZURE_AOAI_CHAT_MODEL_NAME_DEPLOYMENT_ID_TEST'), \n",
    "    api_version=\"2024-05-13\"\n",
    ")\n",
    "tokenizer = AzureOpenAITokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "from requests import Response\n",
    "from threading import Lock\n",
    "\n",
    "# Global variables to store current rate limits\n",
    "# Fake global values obtained from previous calls\n",
    "current_rate_limits = {\n",
    "    \"remaining_requests\": 5,  \n",
    "    \"remaining_tokens\": 300\n",
    "}\n",
    "\n",
    "rate_limit_lock = Lock() # To safely update rate limits across threads\n",
    "successful_requests = 0\n",
    "blocked_requests = 0\n",
    "\n",
    "def update_rate_limits(headers: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Update the global rate limit tracker based on the headers from the response.\n",
    "    \"\"\"\n",
    "    global current_rate_limits\n",
    "    with rate_limit_lock:\n",
    "        current_rate_limits[\"remaining_requests\"] = int(headers.get(\"x-ratelimit-remaining-requests\", 0))\n",
    "        current_rate_limits[\"remaining_tokens\"] = int(headers.get(\"x-ratelimit-remaining-tokens\", 0))\n",
    "        logger.info(f\"Updated rate limits: {current_rate_limits}\")\n",
    "\n",
    "def extract_rate_limit_and_usage_info(response: Response) -> Dict[str, Optional[int]]:\n",
    "    \"\"\"\n",
    "    Extracts rate limiting information from the Azure Open AI response headers and usage information from the payload.\n",
    "\n",
    "    :param response: The response object returned by a requests call.\n",
    "    :return: A dictionary containing the remaining requests, remaining tokens, and usage information.\n",
    "    \"\"\"\n",
    "    headers = response.headers\n",
    "    usage = response.json().get(\"usage\", {})\n",
    "    update_rate_limits(headers)\n",
    "    return {\n",
    "        \"remaining-requests\": int(headers.get(\"x-ratelimit-remaining-requests\", 0)),\n",
    "        \"remaining-tokens\": int(headers.get(\"x-ratelimit-remaining-tokens\", 0)),\n",
    "        \"retry-after\": headers.get(\"retry-after\"),\n",
    "        \"prompt-tokens\": usage.get(\"prompt_tokens\"),\n",
    "        \"completion-tokens\": usage.get(\"completion_tokens\"),\n",
    "        \"total-tokens\": usage.get(\"total_tokens\"),\n",
    "    }\n",
    "\n",
    "def call_azure_openai_chat_completions_api(\n",
    "    deployment_id: str, body: Dict, api_version: str = \"2023-11-01\"\n",
    ") -> Tuple[Optional[int], Optional[Dict], Optional[Dict[str, Optional[int]]]]:\n",
    "    \"\"\"\n",
    "    Calls the Azure OpenAI API with the given parameters.\n",
    "\n",
    "    :param deployment_id: The ID of the deployment to access.\n",
    "    :param body: The body of the request for \"post\" method. Defaults to None.\n",
    "    :param api_version: The API version to use. Defaults to \"2023-11-01\".\n",
    "\n",
    "    :return: The status code and response from the API call, along with rate limit headers.\n",
    "    \"\"\"\n",
    "    url = f\"{azure_openai_client.azure_endpoint}/openai/deployments/{deployment_id}/chat/completions?api-version={api_version}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": azure_openai_client.api_key,\n",
    "        \"x-ms-useragent\": \"aoai-faq\"\n",
    "    }\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update(headers)\n",
    "        response = session.post(url, json=body)\n",
    "        response.raise_for_status()\n",
    "\n",
    "    rate_limit_headers = extract_rate_limit_and_usage_info(response)\n",
    "    logger.debug(f\"Rate limit headers: {rate_limit_headers}\")\n",
    "    return response.status_code, response.json(), rate_limit_headers\n",
    "\n",
    "def can_make_request(body: Dict, model: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if the current request can be made based on the remaining rate limits.\n",
    "\n",
    "    :param body: The body of the request to be sent.\n",
    "    :param model: The model name used for estimating tokens.\n",
    "    :return: True if the request can be made, False otherwise.\n",
    "    \"\"\"\n",
    "    global current_rate_limits\n",
    "    required_tokens = tokenizer.estimate_tokens_azure_openai(messages=body[\"messages\"], model=model)\n",
    "\n",
    "    with rate_limit_lock:\n",
    "        logger.info(f\"Current rate limits before check: {current_rate_limits}\")\n",
    "        logger.info(f\"Required tokens for the request: {required_tokens}\")\n",
    "\n",
    "        can_make_request = True\n",
    "\n",
    "        if current_rate_limits[\"remaining_requests\"] is not None:\n",
    "            if current_rate_limits[\"remaining_requests\"] <= 0:\n",
    "                logger.info(\"Cannot make request: No remaining requests.\")\n",
    "                can_make_request = False\n",
    "            else:\n",
    "                logger.info(f\"Remaining requests: {current_rate_limits['remaining_requests']}\")\n",
    "\n",
    "        if current_rate_limits[\"remaining_tokens\"] is not None:\n",
    "            if current_rate_limits[\"remaining_tokens\"] < required_tokens:\n",
    "                logger.info(\"Cannot make request: Not enough remaining tokens.\")\n",
    "                can_make_request = False\n",
    "            else:\n",
    "                logger.debug(f\"Remaining tokens: {current_rate_limits['remaining_tokens']}\")\n",
    "\n",
    "        if can_make_request:\n",
    "            if current_rate_limits[\"remaining_requests\"] is not None:\n",
    "                current_rate_limits[\"remaining_requests\"] -= 1\n",
    "                logger.info(f\"Decremented remaining requests: {current_rate_limits['remaining_requests']}\")\n",
    "            if current_rate_limits[\"remaining_tokens\"] is not None:\n",
    "                current_rate_limits[\"remaining_tokens\"] -= required_tokens\n",
    "                logger.info(f\"Decremented remaining tokens: {current_rate_limits['remaining_tokens']}\")\n",
    "            logger.info(f\"After decrement, rate limits: {current_rate_limits}\")\n",
    "            return True\n",
    "\n",
    "    logger.info(\"Request cannot be made due to rate limits.\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-04 11:21:08,824 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 5, 'remaining_tokens': 300} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:21:08,826 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:21:08,828 - micro - MainProcess - INFO     Remaining requests: 5 (4183727991.py:can_make_request:97)\n",
      "2024-08-04 11:21:08,829 - micro - MainProcess - INFO     Decremented remaining requests: 4 (4183727991.py:can_make_request:109)\n",
      "2024-08-04 11:21:08,830 - micro - MainProcess - INFO     Decremented remaining tokens: 229 (4183727991.py:can_make_request:112)\n",
      "2024-08-04 11:21:08,832 - micro - MainProcess - INFO     After decrement, rate limits: {'remaining_requests': 4, 'remaining_tokens': 229} (4183727991.py:can_make_request:113)\n",
      "2024-08-04 11:21:12,139 - micro - MainProcess - INFO     Updated rate limits: {'remaining_requests': 9, 'remaining_tokens': 787} (4183727991.py:update_rate_limits:26)\n",
      "2024-08-04 11:21:12,144 - micro - MainProcess - INFO     Status Code: 200 (220847609.py:make_request:9)\n",
      "2024-08-04 11:21:12,147 - micro - MainProcess - INFO     Response: {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}, 'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"Yes, other Azure AI services also support customer-managed keys (CMK). Some of the key Azure AI services that support customer-managed keys include:\\n\\n1. **Azure Cognitive Services** (e.g., Text Analytics, Computer Vision, Speech Services)\\n2. **Azure Machine Learning** \\n3. **Azure Synapse Analytics**\\n\\nThese services typically allow you to use Azure Key Vault to manage your encryption keys, providing you with greater control over the security of your data. Make sure to check specific documentation for the particular service you're using, as support for customer-managed keys may vary between services and regions.\", 'role': 'assistant'}}], 'created': 1722788469, 'id': 'chatcmpl-9sYGjN0Ae2uyHr8wxCU8WwVLx6qd6', 'model': 'gpt-4o-2024-05-13', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'system_fingerprint': 'fp_abc28019ad', 'usage': {'completion_tokens': 119, 'prompt_tokens': 69, 'total_tokens': 188}} (220847609.py:make_request:10)\n",
      "2024-08-04 11:21:12,149 - micro - MainProcess - INFO     Rate Limit Info: {'remaining-requests': 9, 'remaining-tokens': 787, 'retry-after': None, 'prompt-tokens': 69, 'completion-tokens': 119, 'total-tokens': 188} (220847609.py:make_request:11)\n"
     ]
    }
   ],
   "source": [
    "def make_request():\n",
    "    global successful_requests, blocked_requests\n",
    "    if can_make_request(body, \"gpt-4-0613\"):\n",
    "        status_code, response, rate_limit_info = call_azure_openai_chat_completions_api(\n",
    "            deployment_id=azure_openai_client.chat_model_name,\n",
    "            body=body,\n",
    "            api_version=\"2024-02-15-preview\",\n",
    "        )\n",
    "        logger.info(f\"Status Code: {status_code}\")\n",
    "        logger.info(f\"Response: {response}\")\n",
    "        logger.info(f\"Rate Limit Info: {rate_limit_info}\")\n",
    "        with rate_limit_lock:\n",
    "            successful_requests += 1\n",
    "    else:\n",
    "        logger.warning(\"Rate limit exceeded, cannot make the request.\")\n",
    "        with rate_limit_lock:\n",
    "            blocked_requests += 1\n",
    "\n",
    "make_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-04 11:22:00,026 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 9, 'remaining_tokens': 787} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,041 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,044 - micro - MainProcess - INFO     Remaining requests: 9 (4183727991.py:can_make_request:97)\n",
      "2024-08-04 11:22:00,047 - micro - MainProcess - INFO     Decremented remaining requests: 8 (4183727991.py:can_make_request:109)\n",
      "2024-08-04 11:22:00,050 - micro - MainProcess - INFO     Decremented remaining tokens: 716 (4183727991.py:can_make_request:112)\n",
      "2024-08-04 11:22:00,054 - micro - MainProcess - INFO     After decrement, rate limits: {'remaining_requests': 8, 'remaining_tokens': 716} (4183727991.py:can_make_request:113)\n",
      "2024-08-04 11:22:00,060 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 8, 'remaining_tokens': 716} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,067 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,070 - micro - MainProcess - INFO     Remaining requests: 8 (4183727991.py:can_make_request:97)\n",
      "2024-08-04 11:22:00,073 - micro - MainProcess - INFO     Decremented remaining requests: 7 (4183727991.py:can_make_request:109)\n",
      "2024-08-04 11:22:00,077 - micro - MainProcess - INFO     Decremented remaining tokens: 645 (4183727991.py:can_make_request:112)\n",
      "2024-08-04 11:22:00,080 - micro - MainProcess - INFO     After decrement, rate limits: {'remaining_requests': 7, 'remaining_tokens': 645} (4183727991.py:can_make_request:113)\n",
      "2024-08-04 11:22:00,083 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 7, 'remaining_tokens': 645} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,091 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,095 - micro - MainProcess - INFO     Remaining requests: 7 (4183727991.py:can_make_request:97)\n",
      "2024-08-04 11:22:00,098 - micro - MainProcess - INFO     Decremented remaining requests: 6 (4183727991.py:can_make_request:109)\n",
      "2024-08-04 11:22:00,101 - micro - MainProcess - INFO     Decremented remaining tokens: 574 (4183727991.py:can_make_request:112)\n",
      "2024-08-04 11:22:00,104 - micro - MainProcess - INFO     After decrement, rate limits: {'remaining_requests': 6, 'remaining_tokens': 574} (4183727991.py:can_make_request:113)\n",
      "2024-08-04 11:22:00,110 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 6, 'remaining_tokens': 574} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,120 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,122 - micro - MainProcess - INFO     Remaining requests: 6 (4183727991.py:can_make_request:97)\n",
      "2024-08-04 11:22:00,126 - micro - MainProcess - INFO     Decremented remaining requests: 5 (4183727991.py:can_make_request:109)\n",
      "2024-08-04 11:22:00,128 - micro - MainProcess - INFO     Decremented remaining tokens: 503 (4183727991.py:can_make_request:112)\n",
      "2024-08-04 11:22:00,129 - micro - MainProcess - INFO     After decrement, rate limits: {'remaining_requests': 5, 'remaining_tokens': 503} (4183727991.py:can_make_request:113)\n",
      "2024-08-04 11:22:00,131 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 5, 'remaining_tokens': 503} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,141 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,144 - micro - MainProcess - INFO     Remaining requests: 5 (4183727991.py:can_make_request:97)\n",
      "2024-08-04 11:22:00,147 - micro - MainProcess - INFO     Decremented remaining requests: 4 (4183727991.py:can_make_request:109)\n",
      "2024-08-04 11:22:00,149 - micro - MainProcess - INFO     Decremented remaining tokens: 432 (4183727991.py:can_make_request:112)\n",
      "2024-08-04 11:22:00,151 - micro - MainProcess - INFO     After decrement, rate limits: {'remaining_requests': 4, 'remaining_tokens': 432} (4183727991.py:can_make_request:113)\n",
      "2024-08-04 11:22:00,159 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 4, 'remaining_tokens': 432} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,165 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,167 - micro - MainProcess - INFO     Remaining requests: 4 (4183727991.py:can_make_request:97)\n",
      "2024-08-04 11:22:00,168 - micro - MainProcess - INFO     Decremented remaining requests: 3 (4183727991.py:can_make_request:109)\n",
      "2024-08-04 11:22:00,173 - micro - MainProcess - INFO     Decremented remaining tokens: 361 (4183727991.py:can_make_request:112)\n",
      "2024-08-04 11:22:00,175 - micro - MainProcess - INFO     After decrement, rate limits: {'remaining_requests': 3, 'remaining_tokens': 361} (4183727991.py:can_make_request:113)\n",
      "2024-08-04 11:22:00,180 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 3, 'remaining_tokens': 361} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,186 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,203 - micro - MainProcess - INFO     Remaining requests: 3 (4183727991.py:can_make_request:97)\n",
      "2024-08-04 11:22:00,207 - micro - MainProcess - INFO     Decremented remaining requests: 2 (4183727991.py:can_make_request:109)\n",
      "2024-08-04 11:22:00,209 - micro - MainProcess - INFO     Decremented remaining tokens: 290 (4183727991.py:can_make_request:112)\n",
      "2024-08-04 11:22:00,214 - micro - MainProcess - INFO     After decrement, rate limits: {'remaining_requests': 2, 'remaining_tokens': 290} (4183727991.py:can_make_request:113)\n",
      "2024-08-04 11:22:00,219 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 2, 'remaining_tokens': 290} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,226 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,229 - micro - MainProcess - INFO     Remaining requests: 2 (4183727991.py:can_make_request:97)\n",
      "2024-08-04 11:22:00,231 - micro - MainProcess - INFO     Decremented remaining requests: 1 (4183727991.py:can_make_request:109)\n",
      "2024-08-04 11:22:00,232 - micro - MainProcess - INFO     Decremented remaining tokens: 219 (4183727991.py:can_make_request:112)\n",
      "2024-08-04 11:22:00,233 - micro - MainProcess - INFO     After decrement, rate limits: {'remaining_requests': 1, 'remaining_tokens': 219} (4183727991.py:can_make_request:113)\n",
      "2024-08-04 11:22:00,237 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 1, 'remaining_tokens': 219} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,247 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,249 - micro - MainProcess - INFO     Remaining requests: 1 (4183727991.py:can_make_request:97)\n",
      "2024-08-04 11:22:00,251 - micro - MainProcess - INFO     Decremented remaining requests: 0 (4183727991.py:can_make_request:109)\n",
      "2024-08-04 11:22:00,253 - micro - MainProcess - INFO     Decremented remaining tokens: 148 (4183727991.py:can_make_request:112)\n",
      "2024-08-04 11:22:00,254 - micro - MainProcess - INFO     After decrement, rate limits: {'remaining_requests': 0, 'remaining_tokens': 148} (4183727991.py:can_make_request:113)\n",
      "2024-08-04 11:22:00,263 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 0, 'remaining_tokens': 148} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,271 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,282 - micro - MainProcess - INFO     Cannot make request: No remaining requests. (4183727991.py:can_make_request:94)\n",
      "2024-08-04 11:22:00,304 - micro - MainProcess - INFO     Request cannot be made due to rate limits. (4183727991.py:can_make_request:116)\n",
      "2024-08-04 11:22:00,314 - micro - MainProcess - WARNING  Rate limit exceeded, cannot make the request. (220847609.py:make_request:15)\n",
      "2024-08-04 11:22:00,316 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 0, 'remaining_tokens': 148} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,318 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,319 - micro - MainProcess - INFO     Cannot make request: No remaining requests. (4183727991.py:can_make_request:94)\n",
      "2024-08-04 11:22:00,320 - micro - MainProcess - INFO     Request cannot be made due to rate limits. (4183727991.py:can_make_request:116)\n",
      "2024-08-04 11:22:00,327 - micro - MainProcess - WARNING  Rate limit exceeded, cannot make the request. (220847609.py:make_request:15)\n",
      "2024-08-04 11:22:00,330 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 0, 'remaining_tokens': 148} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,331 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,332 - micro - MainProcess - INFO     Cannot make request: No remaining requests. (4183727991.py:can_make_request:94)\n",
      "2024-08-04 11:22:00,333 - micro - MainProcess - INFO     Request cannot be made due to rate limits. (4183727991.py:can_make_request:116)\n",
      "2024-08-04 11:22:00,335 - micro - MainProcess - WARNING  Rate limit exceeded, cannot make the request. (220847609.py:make_request:15)\n",
      "2024-08-04 11:22:00,337 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 0, 'remaining_tokens': 148} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,340 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,342 - micro - MainProcess - INFO     Cannot make request: No remaining requests. (4183727991.py:can_make_request:94)\n",
      "2024-08-04 11:22:00,343 - micro - MainProcess - INFO     Request cannot be made due to rate limits. (4183727991.py:can_make_request:116)\n",
      "2024-08-04 11:22:00,345 - micro - MainProcess - WARNING  Rate limit exceeded, cannot make the request. (220847609.py:make_request:15)\n",
      "2024-08-04 11:22:00,349 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 0, 'remaining_tokens': 148} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,350 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,351 - micro - MainProcess - INFO     Cannot make request: No remaining requests. (4183727991.py:can_make_request:94)\n",
      "2024-08-04 11:22:00,352 - micro - MainProcess - INFO     Request cannot be made due to rate limits. (4183727991.py:can_make_request:116)\n",
      "2024-08-04 11:22:00,353 - micro - MainProcess - WARNING  Rate limit exceeded, cannot make the request. (220847609.py:make_request:15)\n",
      "2024-08-04 11:22:00,354 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 0, 'remaining_tokens': 148} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,357 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,359 - micro - MainProcess - INFO     Cannot make request: No remaining requests. (4183727991.py:can_make_request:94)\n",
      "2024-08-04 11:22:00,360 - micro - MainProcess - INFO     Request cannot be made due to rate limits. (4183727991.py:can_make_request:116)\n",
      "2024-08-04 11:22:00,361 - micro - MainProcess - WARNING  Rate limit exceeded, cannot make the request. (220847609.py:make_request:15)\n",
      "2024-08-04 11:22:00,363 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 0, 'remaining_tokens': 148} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,363 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,364 - micro - MainProcess - INFO     Cannot make request: No remaining requests. (4183727991.py:can_make_request:94)\n",
      "2024-08-04 11:22:00,364 - micro - MainProcess - INFO     Request cannot be made due to rate limits. (4183727991.py:can_make_request:116)\n",
      "2024-08-04 11:22:00,368 - micro - MainProcess - WARNING  Rate limit exceeded, cannot make the request. (220847609.py:make_request:15)\n",
      "2024-08-04 11:22:00,371 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 0, 'remaining_tokens': 148} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,373 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,375 - micro - MainProcess - INFO     Cannot make request: No remaining requests. (4183727991.py:can_make_request:94)\n",
      "2024-08-04 11:22:00,377 - micro - MainProcess - INFO     Request cannot be made due to rate limits. (4183727991.py:can_make_request:116)\n",
      "2024-08-04 11:22:00,379 - micro - MainProcess - WARNING  Rate limit exceeded, cannot make the request. (220847609.py:make_request:15)\n",
      "2024-08-04 11:22:00,383 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 0, 'remaining_tokens': 148} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,384 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,385 - micro - MainProcess - INFO     Cannot make request: No remaining requests. (4183727991.py:can_make_request:94)\n",
      "2024-08-04 11:22:00,386 - micro - MainProcess - INFO     Request cannot be made due to rate limits. (4183727991.py:can_make_request:116)\n",
      "2024-08-04 11:22:00,387 - micro - MainProcess - WARNING  Rate limit exceeded, cannot make the request. (220847609.py:make_request:15)\n",
      "2024-08-04 11:22:00,391 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 0, 'remaining_tokens': 148} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,393 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,394 - micro - MainProcess - INFO     Cannot make request: No remaining requests. (4183727991.py:can_make_request:94)\n",
      "2024-08-04 11:22:00,396 - micro - MainProcess - INFO     Request cannot be made due to rate limits. (4183727991.py:can_make_request:116)\n",
      "2024-08-04 11:22:00,397 - micro - MainProcess - WARNING  Rate limit exceeded, cannot make the request. (220847609.py:make_request:15)\n",
      "2024-08-04 11:22:00,400 - micro - MainProcess - INFO     Current rate limits before check: {'remaining_requests': 0, 'remaining_tokens': 148} (4183727991.py:can_make_request:87)\n",
      "2024-08-04 11:22:00,404 - micro - MainProcess - INFO     Required tokens for the request: 71 (4183727991.py:can_make_request:88)\n",
      "2024-08-04 11:22:00,407 - micro - MainProcess - INFO     Cannot make request: No remaining requests. (4183727991.py:can_make_request:94)\n",
      "2024-08-04 11:22:00,411 - micro - MainProcess - INFO     Request cannot be made due to rate limits. (4183727991.py:can_make_request:116)\n",
      "2024-08-04 11:22:00,412 - micro - MainProcess - WARNING  Rate limit exceeded, cannot make the request. (220847609.py:make_request:15)\n",
      "2024-08-04 11:22:03,280 - micro - MainProcess - INFO     Updated rate limits: {'remaining_requests': 8, 'remaining_tokens': 787} (4183727991.py:update_rate_limits:26)\n",
      "2024-08-04 11:22:03,285 - micro - MainProcess - INFO     Status Code: 200 (220847609.py:make_request:9)\n",
      "2024-08-04 11:22:03,286 - micro - MainProcess - INFO     Response: {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}, 'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Yes, several other Azure AI services also support customer-managed keys. These services typically include:\\n\\n1. **Azure Cognitive Services**: Many of the services under Azure Cognitive Services, such as Text Analytics, Speech Services, and Computer Vision, support customer-managed keys.\\n2. **Azure Machine Learning**: This service allows you to use customer-managed keys to encrypt data stored in Azure Storage, Azure SQL Database, and other linked Azure resources.\\n\\nCustomer-managed keys offer enhanced control over the encryption of your data by allowing you to use keys stored in Azure Key Vault, ensuring that you maintain ownership and control over the encryption keys used by Azure services.', 'role': 'assistant'}}], 'created': 1722788522, 'id': 'chatcmpl-9sYHaFyrcNnh2UYxgwlB89aNT2xga', 'model': 'gpt-4o-2024-05-13', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'system_fingerprint': 'fp_abc28019ad', 'usage': {'completion_tokens': 127, 'prompt_tokens': 69, 'total_tokens': 196}} (220847609.py:make_request:10)\n",
      "2024-08-04 11:22:03,288 - micro - MainProcess - INFO     Rate Limit Info: {'remaining-requests': 8, 'remaining-tokens': 787, 'retry-after': None, 'prompt-tokens': 69, 'completion-tokens': 127, 'total-tokens': 196} (220847609.py:make_request:11)\n",
      "2024-08-04 11:22:03,516 - micro - MainProcess - INFO     Updated rate limits: {'remaining_requests': 8, 'remaining_tokens': 787} (4183727991.py:update_rate_limits:26)\n",
      "2024-08-04 11:22:03,520 - micro - MainProcess - INFO     Status Code: 200 (220847609.py:make_request:9)\n",
      "2024-08-04 11:22:03,521 - micro - MainProcess - INFO     Response: {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}, 'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Yes, several other Azure AI services also support customer-managed keys (CMKs) to provide greater control over encryption and data security. Some of these services include:\\n\\n1. **Azure Cognitive Services**: Supports customer-managed keys for various services under its umbrella, such as Azure Text Analytics, Azure Speech, and Azure Computer Vision.\\n   \\n2. **Azure Machine Learning**: Enables the use of customer-managed keys to encrypt machine learning data including models, datasets, and metadata stored in the Azure Machine Learning workspace.\\n\\n3. **Azure SQL Database with AI features**: Allows customer-managed keys for encrypting data at rest, giving you control over the encryption keys.\\n\\nBy using customer-managed keys, Azure provides greater flexibility and control over data security, meeting compliance requirements and enhancing data protection.', 'role': 'assistant'}}], 'created': 1722788522, 'id': 'chatcmpl-9sYHa1MBFg49p8TXhxMPqZMj463W8', 'model': 'gpt-4o-2024-05-13', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'system_fingerprint': 'fp_abc28019ad', 'usage': {'completion_tokens': 155, 'prompt_tokens': 69, 'total_tokens': 224}} (220847609.py:make_request:10)\n",
      "2024-08-04 11:22:03,525 - micro - MainProcess - INFO     Rate Limit Info: {'remaining-requests': 8, 'remaining-tokens': 787, 'retry-after': None, 'prompt-tokens': 69, 'completion-tokens': 155, 'total-tokens': 224} (220847609.py:make_request:11)\n",
      "2024-08-04 11:22:03,636 - micro - MainProcess - INFO     Updated rate limits: {'remaining_requests': 9, 'remaining_tokens': 3858} (4183727991.py:update_rate_limits:26)\n",
      "2024-08-04 11:22:03,641 - micro - MainProcess - INFO     Status Code: 200 (220847609.py:make_request:9)\n",
      "2024-08-04 11:22:03,643 - micro - MainProcess - INFO     Response: {'choices': [{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}, 'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Yes, several other Azure AI services also support customer-managed keys (CMK) for enhanced security and control over your data. These services include:\\n\\n1. **Azure Machine Learning**: Enables you to use customer-managed keys for data encryption.\\n2. **Azure Cognitive Services**: Many services under this umbrella, such as Text Analytics, Language Understanding (LUIS), and Translator, support customer-managed keys.\\n3. **Azure Cognitive Search**: Supports customer-managed keys for encrypting data in your search service indexes.\\n\\nBy leveraging customer-managed keys, organizations can meet their compliance and security requirements by having more control over the encryption processes used to protect their data.', 'role': 'assistant'}}], 'created': 1722788522, 'id': 'chatcmpl-9sYHaqxEgiC5bZrNgGQMk3SHKdtQI', 'model': 'gpt-4o-2024-05-13', 'object': 'chat.completion', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'system_fingerprint': 'fp_abc28019ad', 'usage': {'completion_tokens': 131, 'prompt_tokens': 69, 'total_tokens': 200}} (220847609.py:make_request:10)\n",
      "2024-08-04 11:22:03,644 - micro - MainProcess - INFO     Rate Limit Info: {'remaining-requests': 9, 'remaining-tokens': 3858, 'retry-after': None, 'prompt-tokens': 69, 'completion-tokens': 131, 'total-tokens': 200} (220847609.py:make_request:11)\n",
      "2024-08-04 11:22:03,650 - micro - MainProcess - INFO     Total successful requests: 4 (2211970555.py:<module>:8)\n",
      "2024-08-04 11:22:03,653 - micro - MainProcess - INFO     Total blocked requests: 11 (2211970555.py:<module>:9)\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# Run parallel calls\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(make_request) for _ in range(20)]\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "logger.info(f\"Total successful requests: {successful_requests}\")\n",
    "logger.info(f\"Total blocked requests: {blocked_requests}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector-indexing-azureaisearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
